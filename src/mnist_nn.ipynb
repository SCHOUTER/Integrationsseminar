{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importieren relevanter Bibliotheken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/train.csv')\n",
    "\n",
    "data = np.array(data)\n",
    "m, n = data.shape\n",
    "np.random.shuffle(data) # shuffle before splitting into dev and training sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datensatz lesen. Konvertieren des Datensatzes in ein zweidimensionales Numpy-Array. Dann mischen wir die Daten, um die Zufälligkeit für das Training zu gewährleisten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dev = data[0:1000].T # Daten aufteilen\n",
    "Y_dev = data_dev[0] # Labels extrahieren\n",
    "X_dev = data_dev[1:n] # Features extrahieren\n",
    "X_dev = X_dev / 255. # Normalisierung\n",
    "\n",
    "data_train = data[1000:m].T # Daten aufteilen\n",
    "Y_train = data_train[0] # Labels extrahieren\n",
    "X_train = data_train[1:n] # Features extrahieren\n",
    "X_train = X_train / 255. # Normalisierung\n",
    "_,m_train = X_train.shape # Größe des Trainingsdatensatzes bestimmen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier teilen wir das Dataset in ein Entwicklungssatz und ein Trainingssatz auf. Die Daten werden auch transponiert, um der erwarteten Eingabeform des neuronalen Netzwerks zu entsprechen. (Features als Spalten und Beispiele als Zeilen) Schließlich werden die Pixelwerte durch Teilen durch 255 normalisiert, sodass sie zwischen 0 und 1 liegen. Diese Normalisierung ist für die Stabilität des Trainings und eine schnellere Konvergenz unerlässlich."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params():\n",
    "    W1 = np.random.rand(10, 784) - 0.5 # Für die erste versteckte Schicht\n",
    "    b1 = np.random.rand(10, 1) - 0.5\n",
    "    W2 = np.random.rand(10, 10) - 0.5  # Für die zweite versteckte Schicht\n",
    "    b2 = np.random.rand(10, 1) - 0.5\n",
    "    W3 = np.random.rand(10, 10) - 0.5  # Für die Ausgabeschicht\n",
    "    b3 = np.random.rand(10, 1) - 0.5\n",
    "    return W1, b1, W2, b2, W3, b3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unser Netzwerk besteht also aus vier Schichten. Die Eingabeschicht, zwei versteckte Schichten, und die Ausgabeschicht. Wir initialisieren die Werte mithilfe von np.random.rand() als Werte zwischen 0 und 1. Indem wir den Bereich auf [-0.5, 0.5] verschieben, erreichen wir eine schneller Konvergenz. (Da zu große Werte dazu führen können, dass der Gradient \"explodiert\" und somit das Training instabil wird, zu kleine Werte dazu, dass der Gradient \"verschwindend\" klein wird und somit das Training sehr langsam wird)\n",
    "\n",
    "\n",
    "W1: 10 Neuronen in der ersten versteckten Schicht, jedes mit 784 Eingangsmerkmalen/Gewichten (eben die 28x28 Pixel)\n",
    "b1: Bias-Werte für die 10 Neuronen der ersten Schicht\n",
    "\n",
    "W2, b2 und W3, b3 ähnlich, ebenfalls 10 Neuronen pro Schicht\n",
    "\n",
    "\n",
    "Die Initiliasierung der Gewichte und Verzerrungen kann einen großen Einfluss darauf haben, wie gut und wie schnell ein neuronales Netzwerk während des Trainings konvergiert. Es gibt viele fortschrittliche Methoden zur Initialisierung, aber der hier gezeigte Ansatz mit kleinen zufälligen Werten ist ein einfacher und oft verwendeter Ansatz, insbesondere für kleinere Netzwerke oder zum Einstieg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(Z):\n",
    "    return np.maximum(Z, 0)\n",
    "\n",
    "def softmax(Z):\n",
    "    A = np.exp(Z) / sum(np.exp(Z))\n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die beiden Aktivierungsfunktionen, die unser Neuronales Netzwerk benutzt:\n",
    "\n",
    "    - ReLU: Benutzen wir für die zwei versteckten Schichten. Alle Werte, die kleiner als 0 sind, werden zu 0, und alle Werte, die größer als 0 sind, bleiben unverändert.\n",
    "\n",
    "    - Softmax: Benutzen wir in der Ausgabeschicht. Sie fungiert als Wahrscheinlichkeitsverteilung, die Werte liegen also zwischen 0 und 1 und die Summe aller Werte ist genau 1. Mit ihrer Hilfe entscheidet sich das Netzwerk in der Ausgabeschicht für eines der 10 Neuronen (Maximum)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(W1, b1, W2, b2, W3, b3, X):\n",
    "    Z1 = W1.dot(X) + b1\n",
    "    A1 = ReLU(Z1)\n",
    "    Z2 = W2.dot(A1) + b2\n",
    "    A2 = ReLU(Z2)  # ReLU für die zweite versteckte Schicht\n",
    "    Z3 = W3.dot(A2) + b3\n",
    "    A3 = softmax(Z3)\n",
    "    return Z1, A1, Z2, A2, Z3, A3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diese Funktion führt die Vorwärtspropagation des Netzwerks durch.\n",
    "\n",
    "Sie berechnet anhand der aktuellen Gewichte und Verzerrungen aller Schichten des Netzwerks die entsprechenden Ausgaben bzw. Aktivierungen für X. Z1 entspricht also dem Produkt aus den Gewichten W1 und der Eingabe X mit anschließender Hinzufügung der Verzerrung b1. Außerdem bestimmen wir dann die Aktivierung A1 der ersten Schicht für X anhand der gewichteten Summe Z1.\n",
    "\n",
    "Das gleiche passiert für die zweite Schicht.\n",
    "\n",
    "Die dritte Schicht unterscheidet sich bloß in der Aktivierungsfunktion, wie bereits erklärt.\n",
    "\n",
    "Als Rückgabe erhalten wir also die gewichteten Summen und Aktivierungen aller Schichten für die Eingabe X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wenn Input (Z) kleiner gleich 0 => false, sonst true\n",
    "def ReLU_deriv(Z):\n",
    "    return Z > 0\n",
    "\n",
    "# 1-aus-n-Code, stellt Dezimalzahlen als Binärzahlen da\n",
    "def one_hot(Y):\n",
    "    one_hot_Y = np.zeros((Y.size, Y.max() + 1))\n",
    "    one_hot_Y[np.arange(Y.size), Y] = 1\n",
    "    one_hot_Y = one_hot_Y.T\n",
    "    return one_hot_Y\n",
    "\n",
    "\n",
    "\n",
    "def backward_prop(Z1, A1, Z2, A2, Z3, A3, W2, W3, X, Y, m):\n",
    "    one_hot_Y = one_hot(Y)\n",
    "    dZ3 = A3 - one_hot_Y\n",
    "    dW3 = 1 / m * dZ3.dot(A2.T)\n",
    "    db3 = 1 / m * np.sum(dZ3, axis=1, keepdims=True)\n",
    "    \n",
    "    dZ2 = W3.T.dot(dZ3) * ReLU_deriv(Z2)\n",
    "    dW2 = 1 / m * dZ2.dot(A1.T)\n",
    "    db2 = 1 / m * np.sum(dZ2, axis=1, keepdims=True)\n",
    "    \n",
    "    dZ1 = W2.T.dot(dZ2) * ReLU_deriv(Z1)\n",
    "    dW1 = 1 / m * dZ1.dot(X.T)\n",
    "    db1 = 1 / m * np.sum(dZ1, axis=1, keepdims=True)\n",
    "    \n",
    "    return dW1, db1, dW2, db2, dW3, db3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Ziel der Rückwertpropagation ist es, zu verstehen, wie sich eine Änderung der Gewichte und Verzerrungen auf den Gesamtfehler des Netzwerks auswirkt. Die zurückgegebenen Werte (dW1, db1, dW2, db2) stellen Gradienten dar, die die Richtung und Größe der zur Fehlerreduktion erforderlichen Änderungen anzeigen. Anders gesagt, gibt der Gradient die Richtung an, in welcher die Funktion, also die Fehlerquote, am steilsten ansteigt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_params(W1, b1, W2, b2, W3, b3, dW1, db1, dW2, db2, dW3, db3, alpha):\n",
    "    W1 -= alpha * dW1\n",
    "    b1 -= alpha * db1\n",
    "    W2 -= alpha * dW2\n",
    "    b2 -= alpha * db2\n",
    "    W3 -= alpha * dW3\n",
    "    b3 -= alpha * db3\n",
    "    return W1, b1, W2, b2, W3, b3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diese Funktion aktualisiert die Gewichte und Verzerrungen des Netzwerks in Richtung des negativen Gradienten. Dieser iterative Prozess hilft dem Netzwerk, aus seinen Fehlern zu lernen. 'alpha' ist die Lernrate, die die Schrittgröße jeder Aktualisierung bestimmt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(A3):\n",
    "    return np.argmax(A3, 0)\n",
    "\n",
    "\n",
    "def get_accuracy(predictions, Y):\n",
    "    print(predictions, Y)\n",
    "    return np.sum(predictions == Y) / Y.size\n",
    "\n",
    "\n",
    "def gradient_descent(X, Y, alpha, iterations):\n",
    "    W1, b1, W2, b2, W3, b3 = init_params()\n",
    "    for i in range(iterations):\n",
    "        Z1, A1, Z2, A2, Z3, A3 = forward_prop(W1, b1, W2, b2, W3, b3, X)\n",
    "        dW1, db1, dW2, db2, dW3, db3 = backward_prop(Z1, A1, Z2, A2, Z3, A3, W2, W3, X, Y, m_train)\n",
    "        W1, b1, W2, b2, W3, b3 = update_params(W1, b1, W2, b2, W3, b3, dW1, db1, dW2, db2, dW3, db3, alpha)\n",
    "        if i % 10 == 0:\n",
    "            print(\"Iteration: \", i)\n",
    "            predictions = get_predictions(A3)\n",
    "            print(get_accuracy(predictions, Y))\n",
    "    return W1, b1, W2, b2, W3, b3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diese Funktion führt das Training unseres neuronalen Netzwerks durch. Es verwendet den Gradientenabstieg, um die Gewichte und Verzerrungen kontinuierlich zu aktualisieren. Am Ende des Trainings sollte unser Netzwerk besser auf die Daten abgestimmt sein und genauere Vorhersagen treffen können.\n",
    "\n",
    "Die Analogie mit einer Kugel, die einen Hang hinunterrollt:\n",
    "\n",
    "    - Die Kugel repräsentiert unsere aktuelle Position (oder den aktuellen Wert der Gewichte) im Fehlerlandschaft.\n",
    "    \n",
    "    - Der Hang repräsentiert die Verlustfunktion.\n",
    "    \n",
    "    - Die Schwerkraft zwingt die Kugel dazu, den Weg des geringsten Widerstands zu suchen und sich bergab zu bewegen.\n",
    "\n",
    "\n",
    "    - Eine hohe Lernrate (α) würde bedeuten, dass die Kugel einen großen Sprung bergab macht. Dies könnte zwar schneller zum Tal (dem Minimum) führen, birgt aber auch das Risiko, dass die Kugel das Tal überquert und auf der anderen Seite wieder nach oben rollt. In anderen Worten, eine zu hohe Lernrate kann dazu führen, dass der Algorithmus \"überschwingt\" und nicht konvergiert.\n",
    "    \n",
    "    - Eine niedrige Lernrate würde bedeuten, dass die Kugel kleinere Schritte bergab macht. Dies kann dazu führen, dass die Kugel sicherer und stetiger zum Tal gelangt, aber es könnte sehr lange dauern. Eine zu niedrige Lernrate kann den Trainingsprozess erheblich verlangsamen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  0\n",
      "[7 6 5 ... 5 5 3] [5 8 5 ... 7 8 4]\n",
      "0.10492682926829268\n",
      "Iteration:  10\n",
      "[7 2 2 ... 7 3 2] [5 8 5 ... 7 8 4]\n",
      "0.11146341463414634\n",
      "Iteration:  20\n",
      "[7 2 2 ... 7 3 2] [5 8 5 ... 7 8 4]\n",
      "0.18885365853658537\n",
      "Iteration:  30\n",
      "[7 4 2 ... 5 6 6] [5 8 5 ... 7 8 4]\n",
      "0.2374390243902439\n",
      "Iteration:  40\n",
      "[1 4 0 ... 5 6 4] [5 8 5 ... 7 8 4]\n",
      "0.3024390243902439\n",
      "Iteration:  50\n",
      "[1 4 0 ... 5 6 4] [5 8 5 ... 7 8 4]\n",
      "0.3987317073170732\n",
      "Iteration:  60\n",
      "[1 4 0 ... 5 7 4] [5 8 5 ... 7 8 4]\n",
      "0.46070731707317075\n",
      "Iteration:  70\n",
      "[1 4 0 ... 5 5 4] [5 8 5 ... 7 8 4]\n",
      "0.5155365853658537\n",
      "Iteration:  80\n",
      "[1 4 0 ... 5 8 4] [5 8 5 ... 7 8 4]\n",
      "0.5627560975609756\n",
      "Iteration:  90\n",
      "[1 4 0 ... 5 8 4] [5 8 5 ... 7 8 4]\n",
      "0.597609756097561\n",
      "Iteration:  100\n",
      "[1 7 0 ... 5 8 4] [5 8 5 ... 7 8 4]\n",
      "0.6253658536585366\n",
      "Iteration:  110\n",
      "[1 7 0 ... 5 8 4] [5 8 5 ... 7 8 4]\n",
      "0.646609756097561\n",
      "Iteration:  120\n",
      "[1 7 0 ... 5 8 4] [5 8 5 ... 7 8 4]\n",
      "0.6659024390243903\n",
      "Iteration:  130\n",
      "[1 7 0 ... 5 8 4] [5 8 5 ... 7 8 4]\n",
      "0.6827317073170732\n",
      "Iteration:  140\n",
      "[1 7 0 ... 5 8 4] [5 8 5 ... 7 8 4]\n",
      "0.6972926829268292\n",
      "Iteration:  150\n",
      "[1 7 5 ... 5 8 4] [5 8 5 ... 7 8 4]\n",
      "0.7096829268292683\n",
      "Iteration:  160\n",
      "[1 8 5 ... 5 8 4] [5 8 5 ... 7 8 4]\n",
      "0.7205609756097561\n",
      "Iteration:  170\n",
      "[1 8 5 ... 5 8 4] [5 8 5 ... 7 8 4]\n",
      "0.7315853658536585\n",
      "Iteration:  180\n",
      "[1 8 5 ... 5 8 4] [5 8 5 ... 7 8 4]\n",
      "0.7411219512195122\n",
      "Iteration:  190\n",
      "[1 8 5 ... 5 8 4] [5 8 5 ... 7 8 4]\n",
      "0.7491463414634146\n",
      "Iteration:  200\n",
      "[1 8 5 ... 5 8 4] [5 8 5 ... 7 8 4]\n",
      "0.755780487804878\n",
      "Iteration:  210\n",
      "[1 8 5 ... 5 8 4] [5 8 5 ... 7 8 4]\n",
      "0.7623658536585366\n",
      "Iteration:  220\n",
      "[1 8 5 ... 5 8 4] [5 8 5 ... 7 8 4]\n",
      "0.7683658536585366\n",
      "Iteration:  230\n",
      "[1 8 5 ... 5 8 4] [5 8 5 ... 7 8 4]\n",
      "0.773390243902439\n",
      "Iteration:  240\n",
      "[1 8 5 ... 5 8 4] [5 8 5 ... 7 8 4]\n",
      "0.7779756097560976\n",
      "Iteration:  250\n",
      "[1 8 5 ... 5 8 4] [5 8 5 ... 7 8 4]\n",
      "0.781829268292683\n",
      "Iteration:  260\n",
      "[1 8 5 ... 5 8 4] [5 8 5 ... 7 8 4]\n",
      "0.7861463414634147\n",
      "Iteration:  270\n",
      "[1 8 5 ... 5 8 4] [5 8 5 ... 7 8 4]\n",
      "0.7901707317073171\n",
      "Iteration:  280\n",
      "[1 8 5 ... 5 8 4] [5 8 5 ... 7 8 4]\n",
      "0.7935853658536586\n",
      "Iteration:  290\n",
      "[1 8 5 ... 5 8 4] [5 8 5 ... 7 8 4]\n",
      "0.7972439024390244\n"
     ]
    }
   ],
   "source": [
    "W1, b1, W2, b2, W3, b3 = gradient_descent(X_train, Y_train, 0.10, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(X, W1, b1, W2, b2, W3, b3):\n",
    "    _, _, _, _, _, A3 = forward_prop(W1, b1, W2, b2, W3, b3, X)\n",
    "    predictions = get_predictions(A3)\n",
    "    return predictions\n",
    "\n",
    "def test_prediction(index, W1, b1, W2, b2, W3, b3):\n",
    "    current_image = X_train[:, index, None]\n",
    "    prediction = make_predictions(X_train[:, index, None], W1, b1, W2, b2, W3, b3)\n",
    "    label = Y_train[index]\n",
    "    print(\"Prediction: \", prediction)\n",
    "    print(\"Label: \", label)\n",
    "    \n",
    "    current_image = current_image.reshape((28, 28)) * 255\n",
    "    plt.gray()\n",
    "    plt.imshow(current_image, interpolation='nearest')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prediction(0, W1, b1, W2, b2, W3, b3)\n",
    "test_prediction(1, W1, b1, W2, b2, W3, b3)\n",
    "test_prediction(2, W1, b1, W2, b2, W3, b3)\n",
    "test_prediction(3, W1, b1, W2, b2, W3, b3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_predictions = make_predictions(X_dev, W1, b1, W2, b2, W3, b3)\n",
    "get_accuracy(dev_predictions, Y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_wrong_predictions(X, Y, dev_predictions, W1, b1, W2, b2, W3, b3, num_samples=10):\n",
    "    wrong_indices = np.where(dev_predictions != Y)[0]  # Find where predictions don't match the actual labels\n",
    "    displayed = 0\n",
    "    \n",
    "    for index in wrong_indices:\n",
    "        if displayed >= num_samples:  # Limit the number of displayed images to num_samples\n",
    "            break\n",
    "\n",
    "        current_image = X[:, index, None]\n",
    "        predicted_label = dev_predictions[index]\n",
    "        actual_label = Y[index]\n",
    "\n",
    "        print(\"Predicted Label:\", predicted_label)\n",
    "        print(\"Actual Label:\", actual_label)\n",
    "\n",
    "        current_image = current_image.reshape((28, 28)) * 255\n",
    "        plt.gray()\n",
    "        plt.imshow(current_image, interpolation='nearest')\n",
    "        plt.show()\n",
    "\n",
    "        displayed += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(image_path, W1, b1, W2, b2, W3, b3):\n",
    "    # Load the image\n",
    "    image = Image.open(image_path).convert(\"L\")  # Convert image to grayscale\n",
    "    image = image.resize((28, 28))  # Resize image to match input size\n",
    "    \n",
    "    # Convert image to numpy array and normalize\n",
    "    image_arr = np.array(image) / 255.0\n",
    "    \n",
    "    # Reshape the image to be a flat vector and then a column vector\n",
    "    image_arr = image_arr.reshape((-1, 1))\n",
    "    \n",
    "    # Predict\n",
    "    prediction = make_predictions(image_arr, W1, b1, W2, b2, W3, b3)\n",
    "    \n",
    "    # Display the image\n",
    "    plt.gray()\n",
    "    plt.imshow(np.array(image), interpolation='nearest')\n",
    "    plt.title(f\"Predicted Label: {prediction[0]}\")\n",
    "    plt.show()\n",
    "\n",
    "    return prediction[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_wrong_predictions(X_dev, Y_dev, dev_predictions, W1, b1, W2, b2, W3, b3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"../data/7_black.png\"\n",
    "predict_image(image_path, W1, b1, W2, b2, W3, b3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
